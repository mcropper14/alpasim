{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "import dataclasses\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import sys\n",
    "from typing import Any\n",
    "\n",
    "import polars as pl\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval import kratos_utils\n",
    "from eval.aggregation import eval_aggregation\n",
    "from eval.data import EvaluationResultContainer\n",
    "from eval.schema import EvalConfig\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "logger = logging.getLogger(\"alpasim_eval_dev_notebook\")\n",
    "from eval.main import *\n",
    "import numpy as np\n",
    "import shapely\n",
    "from rich import print\n",
    "import rich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"asl_search_glob\": \"/home/migl/workspace/alpasim/.wizard/asl/clipgt-d8cbf4ca-b7ff-44bd-a5be-260f736a02fe/15f2c488-10ad-11f0-b123-0242c0a84004/**/*.asl\",\n",
    "    # \"config_path\": \"/home/migl/workspace/alpasim/.wizard/eval-config.yaml\",\n",
    "    \"config_path\": \"/home/migl/workspace/alpasim/src/eval/example_config.yaml\",\n",
    "    \"output_dir\": \"/home/migl/workspace/alpasim/.wizard/eval\",\n",
    "}\n",
    "\n",
    "args = DictConfig(args)\n",
    "\n",
    "config_untyped = OmegaConf.load(args.config_path)\n",
    "cfg: EvalConfig = OmegaConf.merge(EvalConfig, config_untyped)\n",
    "cfg.database.upload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "logger.info(\"Config details:\")\n",
    "logger.info(pprint.pformat(OmegaConf.to_container(cfg, resolve=True)))\n",
    "\n",
    "# Option 2: Use the current event loop\n",
    "# evaluation_result_containers = asyncio.run(get_evaluation_result_containers(args, cfg))\n",
    "evaluation_result_containers = await get_evaluation_result_containers(args, cfg)\n",
    "\n",
    "evaluation_result_containers = get_metric_results(evaluation_result_containers, cfg)\n",
    "\n",
    "run_metadata = kratos_utils.get_metadata(pathlib.Path(args.config_path))\n",
    "# Contains: timestamp_us, value, valid, metric_name, clipgt_id, batch_id, rollout_id\n",
    "df = pl.concat(\n",
    "    [\n",
    "        df_from_evaluation_result_container(eval_result_container, run_metadata)\n",
    "        for eval_result_container in evaluation_result_containers\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(f\"Writing metrics df to {args.output_dir}/metrics.parquet\")\n",
    "df.write_parquet(os.path.join(args.output_dir, \"metrics.parquet\"))\n",
    "processed_dfs = eval_aggregation.aggregate_and_write_metrics_results_txt(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_timestamps[0] / 1e6, filtered_timestamps[-1] / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = full_gt_trajectory\n",
    "t2 = full_gt_trajectory.interpolate_to_timestamps(\n",
    "    simulation_result.timestamps_us[ts_idx : ts_idx + 1]\n",
    ")\n",
    "# print(t1.time_range_us.stop)\n",
    "# print(t2.time_range_us.stop)\n",
    "print(t1.poses.vec3[-1])\n",
    "print(t2.poses.vec3[-1])\n",
    "\n",
    "print(list(full_gt_linestring.coords)[-1])\n",
    "print(list(current_gt_linestring.coords)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_gt_linestring.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_gt_linestring.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.scorers.wrong_lane import *\n",
    "\n",
    "wrong_lane_scorer = WrongLaneScorer(cfg)\n",
    "wrong_lane_scorer.debug_plot(simulation_result, ts, \"curr_lanes\")\n",
    "wrong_lane_scorer.debug_plot(simulation_result, ts, \"possible_current_lanes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for lane_polygon in curr_lanes:\n",
    "    shapely_plotting.plot_polygon(get_lane_polygon(lane_polygon), ax=ax)\n",
    "shapely_plotting.plot_polygon(ego_polygon, ax=ax)\n",
    "\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_lanes[0].center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_lane_idx = np.where(ego_in_lane)[0][0]\n",
    "\n",
    "\n",
    "lane_polygon = get_lane_polygon(lane)\n",
    "print(lane_polygon.contains(ego_polygon.centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapely.LineString(lane.center.points[..., :2]).buffer(1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
