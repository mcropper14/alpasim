# \/ defines order of inheritance of configs in hydra. `config_schema` is defined in python code (not .yaml)
# and serves as the base skeleton. `_self_` means this file is applied on top of that schema.
defaults:
  - config_schema
  - _self_
  - stable_manifest/oss

# \/ hydra-specific configuration of legacy behavior (off)
hydra:
  job:
    chdir: false

# This dictionary serves only to be references in other parts of the config file
defines:
  filesystem: ???
  driver_model: ???

  # defaults will work out of the box \/
  vavam_driver: "${defines.filesystem}/vavam-driver"
  sensordata: "${defines.filesystem}/nre-artifacts"

  helper: scripts
  vscode: sources/remote-vscode-server
  nre_cache_size: 4

# \/ the wizard.* section defines properties global to all alpasim modules
wizard:
  # Name of the run, used to identify the run in the databases. If null, will
  # use the SLURM job name if set.
  run_name: null
  # change to `SLURM` for ORD deployments, `DOCKER_COMPOSE` for local runs via docker-compose and `DOCKER` for local runs via "raw" docker
  run_method: "DOCKER_COMPOSE"

  # directory path to log the results of this run. ??? indicates it is mandatory to override this via cmdline args
  log_dir: ???

  # with wizard.dry_run=true we wouldn't start any containers and just print out the commands that would be executed.
  # note this performs various checks interanlly and thus may still not work if you're not on an ORD node
  dry_run: false

  # assert that all defined volume mounts exist at the start of the simulation
  validate_mount_points: true

  # the directory which will be searched for `.sqsh` files with images needed for simulation when running on ORD with
  # `wizard.run_method=SLURM`. The default location should contain all files required for running alpasim versions from
  # the repo but you may need to add your own cache if you're playing with custom-built images.
  sqshcaches:
    - "${defines.filesystem}/sqsh"
    - "${defines.filesystem}/sqsh/dev"

  # the different containers will use ports {baseport}, {baseport+1}, ...
  # usually not needed to modify
  baseport: 6000

  # Does not need to be touched. For slurm runs (wizard.run_method=SLURM) this will be replaced with the slurm job ID
  slurm_job_id: null

  # Services to run for simulation (e.g., ["driver", "sensorsim", "physics", "trafficsim", "controller", "runtime"])
  run_sim_services:
    ["driver", "sensorsim", "physics", "trafficsim", "controller", "runtime"]

  # Services to run for evaluation (e.g., ["eval"])
  run_eval_services: ["eval"]

  # Services to run for aggregation (e.g., ["post_eval_aggregation"])
  run_aggregation_services: ["post_eval_aggregation"]

  # TODO: undocumented options
  run_mode: "BATCH"
  timeout: 600
  latest_symlink: false

  # If set, the wizard will pull the driver code from the specified hash into
  # `${wizard.log_dir}/driver_code`. Can be useful for mounting into the
  # driver container for debugging.
  driver_code_hash: null

  # Used if `driver_code_hash` is set. Requires configured ssh keys for
  # pulling from gitlab, but can also point towards a local repo!
  driver_code_repo: "ssh://git@gitlab-master.nvidia.com:12051/alpamayo/alpamayo.git"

  submitter: "unspecified" # An optional specifier of the submitter (used for tracking purposes)
  description: "unspecified" # An optional description of the run (used for tracking purposes)

scenes:
  # `scene_ids` is defined in stable_manifest/oss.yaml
  source: "local" # "local" only for OSS at the moment

  artifact_compatibility_matrix:
    # 1. the keys replace dots with underscores because of hydra limitations. They will be interpreted with `.replace('_', '.')`
    # 2. a version a.b.c-deadbeef should not self-reference (include a_b_c-deadbeef) - this is a configuration error.
    # 3. we use a dictionary instead of a list to enable easy partial overrides in hydra
    #    * to add a new compatible version ad-hoc write `scenes.artifact_compatibility_matrix.current_version.new_version=true`
    #    * to remove, write `scenes.artifact_compatibility_matrix.current_version.new_version=false`
    "0_2_0":
      "25_7_9-b85d0efd": true
      "25_7_9-e633dd23": true
      "25_7_8-fc8b0551": true
      "0_2_578-e82c0193": true

  database:
    scene_cache: "${defines.filesystem}/nre-artifacts"

  local:
    directory: ??? # relative path to the nre-artifacts directory
    suites: null # optional: dictionary mapping suite names to lists of scene IDs

# \/ services.* defines the individual components of the simulation. Each of them is deployed from an image so the layout of
# each item is similar. For the services that pull in local code (e.g., controller, runtime, eval) we mount the repo-relative
# `src/` directory into `/mnt/src` in the container and run from there. The virtual environment is reused from the container.
services:
  # \/ sensor simulator for now is just NRE. We'll use it as an example on the structure of a container definition
  sensorsim:
    image: ???
    # \/ volumes lets us mount host (ORD/local) containers to the running container
    volumes:
      - "${scenes.database.scene_cache}:/mnt/nre-data"
      - "${defines.sensordata}/ego-hoods:/mnt/ego-hoods"
    # \/ environments lets you set environment variables inside the container
    environments:
      # this may not be necessary but at least on COLOSSUS by default pytorch had really stupid
      # configuration running to excessively parallelizing everything
      - OMP_NUM_THREADS=1
    # \/ command is like docker entrypoint + command combined
    command:
      - "/app/pycena_run.runfiles/nre_repo/scripts/pycena/runtime/entrypoint_3_11.sh"
      - "--port={port}" # {port} is a wizard-generated unique variable which enumerates {baseport}, {baseport+1}, ...
      - "--host=0.0.0.0" # the default container IP
      - "--artifact-glob=/mnt/nre-data/{sceneset}/**/*.usdz"
      - "--egocar-hood-dir=/mnt/ego-hoods"
      - "--no-enable-nrend"
      - "--download-cache-dir /tmp/nre-cache-dir" # unused
      - "--cache-size=${defines.nre_cache_size}" # as a rule of thumb n_concurrent_rollouts + 1 allows to avoid premature evictions
      # - "--enable-timing" # uncomment to enable timing information on the sensor simulator side
    # \/ gpus is a list of GPUs to use for all service replicas, each replica using one (possibly shared GPU). See below
    gpus: [2, 3, 4, 5, 6, 7]
    # \/ replicas indicates we want 6 identical containers started (for load balancing). With the `gpus` definition above we'll
    # have 6 replicas on 6 GPUs but it's possible to e.g. have 12 replicas on 6 GPUs (to better utilize them).
    replicas: 6

  # \/ services.drive is the driving policy (Alpamayo/HydraMDP, etc)
  driver:
    image: ???
    volumes:
      - "${defines.vavam_driver}:/mnt/vavam_driver" # alpackage location
      - "${wizard.log_dir}:/mnt/output" # optional output like the debug images
      - "${repo-relative:'src'}:/repo/src"
    command:
      - "uv run -m alpasim_driver.main"
      - "--config-path=/mnt/output"
      - "--config-name=driver-config.yaml"
      - "host=0.0.0.0"
      - "port={port}"

    replicas: 1
    gpus: [0]

  physics:
    image: ???
    volumes:
      - "${scenes.database.scene_cache}:/mnt/nre-data"
      - "${repo-relative:'src'}:/repo/src"
    environments:
      - WARP_CACHE_PATH='/mnt/warp'
    command:
      - "uv run physics_server"
      - "--host=0.0.0.0" # again, default container IP
      - "--port={port}" # arbitrary, needs to be updated in runtime-config.yaml
      - "--artifact-glob=/mnt/nre-data/{sceneset}/**/*.usdz"
      - "--use-ground-mesh=true"
    replicas: 1
    gpus: [1]

  trafficsim:
    image: ???
    command:
      - "echo 'not yet included'"
    replicas: 1
    gpus: null

  controller:
    image: ???
    volumes:
      - "${wizard.log_dir}/controller:/mnt/output" # output like the roadcast logs
      - "${repo-relative:'src'}:/repo/src"
    command:
      - "uv run python -m alpasim_controller.server"
      - "--port={port}"
      - "--log_dir=/mnt/output"
    replicas: 1
    gpus: null


  # \/ the orchestrating container - only 1 needed irrespectively of however many other there are
  runtime:
    image: ???
    # \/ only starts if all the dependencies started correctly
    depends_on:
      - driver
      - sensorsim
      - physics
      - trafficsim
      - controller
    volumes:
      - "${scenes.database.scene_cache}:/mnt/nre-data"
      - "${wizard.log_dir}:/mnt/rollouts"
      - "${repo-relative:'src'}:/repo/src"
    gpus: null # uses no GPUs
    command:
      - "uv run python -m alpasim_runtime.simulate"
      - "--usdz-glob=/mnt/nre-data/{sceneset}/**/*.usdz"
      - "--prometheus-out=/mnt/rollouts/alpasim-runtime.prom"
      - "--user-config=/mnt/rollouts/{runtime_config_name}"
      - "--network-config=/mnt/rollouts/generated-network-config.yaml"

  eval:
    image: ???
    volumes:
      - "${wizard.log_dir}:/mnt/log_dir"
      - "${scenes.database.scene_cache}:/mnt/nre-data"
      - "${repo-relative:'src'}:/repo/src"
    replicas: 1
    gpus: null
    command:
      - "uv run alpasim-eval"
      - "--asl_search_glob=/mnt/log_dir/asl/**/*.asl"
      - "--config_path=/mnt/log_dir/eval-config.yaml"
      - "--output_dir=/mnt/log_dir/eval"
      - "--trajdata_cache_dir=/mnt/trajdata_cache_dir"
      - "--usdz_glob=/mnt/nre-data/{sceneset}/**/*.usdz"

  post_eval_aggregation:
    image: ${services.eval.image}
    volumes:
      - "${wizard.log_dir}:/mnt/log_dir"
      - "${or:${wizard.array_job_dir},${wizard.log_dir}}:/mnt/array_job_dir"
      - "${repo-relative:'src'}:/repo/src"
    replicas: 1
    gpus: null
    command:
      - "uv run alpasim-aggregation"
      - "--array_job_dir=/mnt/array_job_dir"
      - "--config_path=/mnt/log_dir/eval-config.yaml"

runtime:
  endpoints:
    sensorsim:
      # how many rollouts can run on a single worker at once
      n_concurrent_rollouts: 2

    driver:
      n_concurrent_rollouts: 14

    physics:
      n_concurrent_rollouts: 14
      skip: false # physics supports skipping

    controller:
      n_concurrent_rollouts: 14
      skip: false # controller supports skipping

    trafficsim:
      n_concurrent_rollouts: 14
      skip: true # trafficsim is not included yet

    # shut down the system after simulation is finished. without this flag the microservice servers
    # will remain on forever requring a manual interrupt (useful for debugging)
    do_shutdown: true
    # Cache size used by the scene cache monitor to ensure enough capacity
    sensorsim_cache_size: ${defines.nre_cache_size}

  save_dir: "/mnt/rollouts/asl" # this is tied to mount points in `docker-compose.yaml`
  enable_autoresume: false

  default_scenario_parameters:
    n_sim_steps: 200 # how many steps to simulate in a rollout
    n_rollouts: 1 # how many rollouts to simulate for that scenario
    force_gt_duration_us: 1_700_000 # 1.7s
    send_recording_ground_truth: false # Disable sending ground truth data to the driver
    send_av_messages: false # Flag to enable sending AV messages to the driver

    egopose_interval_us: 100_000
    control_timestep_us: 100_000

    time_start_offset_us: 300_000

    ego_mask_rig_config_id: "hyperion_8_1" # which rig/directory in ego-hoods to use for ego masking
    planner_delay_us: ???  # Overwritten in `oss.yaml` or `internal.yaml`

    # Actors present shorter than this (in micro-seconds) are ignored. Set to 0 to disable.
    min_traffic_duration_us: 3_000_000

    assert_zero_decision_delay: true

    physics_update_mode: "EGO_ONLY"

    egomotion_noise: # orders of magnitude loosely based on
      # https://docs.google.com/spreadsheets/d/1W9y6uHFOqouLx-K7xp3gh43WaAYb3Z9uYckeCA2wCgw/edit?gid=1620097033#gid=1620097033
      enabled: False # Note, if setting to true, it is likely that controller noise should be disabled to prevent double booking
      cov_x: 0.05
      cov_y: 0.05
      cov_z: 0.0
      time_constant_position: 3.0
      cov_orientation_x: 0.0
      cov_orientation_y: 0.0
      cov_orientation_z: 0.0007
      time_constant_orientation: 5.0

    route_generator_type: "MAP" # "MAP" or "RECORDED"

    vehicle: null # use values from the .usdz file (original dimensions)

    # Note: If you change the `frame_interval_us`, it will affect the frequency of
    # camera images sent to the driver, which, depending on the driver, could effect
    # closed loop behavior. If you want to keep the behavior unchanged, consider
    # asjusting the `driver.inference.Cframes_subsample` parameter,
    # which controls how many frames are skipped between consecutive images sent to the driver.
    cameras:
      - height: 320
        width: 512
        logical_id: camera_front_wide_120fov

        # If you change this see note above about `driver.inference.Cframes_subsample`
        frame_interval_us: 100_000
        shutter_duration_us: 30_000
        first_frame_offset_us: -30_000
      - height: 320
        width: 512
        logical_id: camera_front_tele_30fov

        # If you change this see note above about `driver.inference.Cframes_subsample`
        frame_interval_us: 100_000
        shutter_duration_us: 30_000
        first_frame_offset_us: -30_000

eval:
  vec_map:
    incl_road_edges: true
    incl_traffic_signs: true
    incl_wait_lines: true
    max_num_lanes: 20
    num_pts_per_lane: 20

  num_processes: 16
  vehicle:
    vehicle_corner_roundness: 0.5
    vehicle_shrink_factor: 0.02

  scorers:
    min_ade:
      time_deltas: [0.5, 1.0, 2.5, 5.0]
      incl_z: False
      target: GT
    plan_deviation:
      incl_z: False
      avg_decay_rate: 0.1
      min_timesteps: 5
    image:
      camera_logical_id: camera_front_wide_120fov

  aggregation_modifiers:
    max_dist_to_gt_trajectory: 4.0

  video:
    render_video: True
    camera_id_to_render: camera_front_wide_120fov
    render_every_nth_frame: 1
    generate_combined_video: False
    combined_video_speed_factor: 0.33
    map_video:
      map_radius_m: 20
      ego_loc: BOTTOM_CENTER
      rotate_map_to_ego: True
      map_elements_to_plot:
        - ROAD_LANE_CENTER
        - ROAD_LANE_LEFT_EDGE
        - ROAD_LANE_RIGHT_EDGE
        - ROAD_EDGE
        - STOP_LINE
        - GT_LINESTRING
        - EGO_GT_GHOST_POLYGON
        - DRIVER_RESPONSES
        - ROUTE
        - AGENTS

driver: ???
