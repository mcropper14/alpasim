# SPDX-License-Identifier: Apache-2.0
# Copyright (c) 2025 NVIDIA Corporation

"""
The main entrypoint to start simulations with alpasim.
"""

from __future__ import annotations

import argparse
import asyncio
import concurrent.futures
import logging
import multiprocessing as mp
import os
import random
import sys
import time
import traceback

from alpasim_runtime.autoresume import (
    find_num_complete_rollouts,
    remove_incomplete_rollouts,
)
from alpasim_runtime.camera_catalog import CameraCatalog
from alpasim_runtime.config import (
    NetworkSimulatorConfig,
    SimulatorConfig,
    UserSimulatorConfig,
    typed_parse_config,
)
from alpasim_runtime.dispatcher import Dispatcher
from alpasim_runtime.loop import UnboundRollout
from alpasim_runtime.metrics import dump_prometheus_metrics
from alpasim_runtime.scene_cache_monitor import SceneCacheMonitor
from alpasim_utils.artifact import Artifact

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s.%(msecs)03d %(levelname)s:\t%(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


def parse_config(user_config_path: str, network_config_path: str) -> SimulatorConfig:
    """
    Parses the user-generated config and the networking config (usually autogenerated) and assembles
    them into a single config struct
    """
    user_config = typed_parse_config(user_config_path, UserSimulatorConfig)
    network_config = typed_parse_config(network_config_path, NetworkSimulatorConfig)

    return SimulatorConfig(user=user_config, network=network_config)


def create_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()

    # we split user and network config files because the latter is commonly auto-generated by kubernetes
    parser.add_argument("--user-config", type=str, required=True)
    parser.add_argument("--network-config", type=str, required=True)
    parser.add_argument("--usdz-glob", type=str, required=True)
    parser.add_argument(
        "--prometheus-out",
        type=str,
        required=False,
        default=None,
    )

    return parser


async def aio_main(args: argparse.Namespace) -> bool:

    config = parse_config(args.user_config, args.network_config)

    artifacts = Artifact.discover_from_glob(
        args.usdz_glob, smooth_trajectories=config.user.smooth_trajectories
    )

    logger.info(f"Available artifacts: {list(artifacts.keys())}.")

    camera_catalog = CameraCatalog(config.user.extra_cameras)
    sensorsim_scene_cache_monitor = SceneCacheMonitor(
        config.user.endpoints.sensorsim_cache_size
    )

    dispatcher = await Dispatcher.create(
        user_config=config.user.endpoints,
        network_config=config.network,
        camera_catalog=camera_catalog,
        sensorsim_scene_cache_monitor=sensorsim_scene_cache_monitor,
    )

    version_ids = dispatcher.gather_version_ids()

    rollout_futures: list[concurrent.futures.Future] = []
    rollouts: list[UnboundRollout] = []

    error_messages: list[str] = []
    future_to_scene = {}

    physical_cores = os.cpu_count()

    ctx = mp.get_context("spawn")

    with concurrent.futures.ProcessPoolExecutor(
        max_workers=2 * physical_cores, mp_context=ctx
    ) as executor:
        for scenario in config.user.scenarios:
            error_messages.extend(
                await dispatcher.find_scenario_incompatibilities(scenario)
            )

            n_rollouts_to_dispatch = scenario.n_rollouts
            if config.user.enable_autoresume:
                remove_incomplete_rollouts(config.user.save_dir, scenario.scene_id)
                num_finished_rollouts = find_num_complete_rollouts(
                    config.user.save_dir, scenario.scene_id
                )
                if num_finished_rollouts != 0:
                    logger.info(
                        f"Found {num_finished_rollouts} already completed rollouts for {scenario.scene_id=}"
                    )
                    n_rollouts_to_dispatch -= num_finished_rollouts

            while n_rollouts_to_dispatch > 0:
                submission = executor.submit(
                    UnboundRollout.create,
                    config=config.user,
                    scenario=scenario,
                    version_ids=version_ids,
                    random_seed=random.randint(0, 2**32 - 1),
                    available_artifacts=artifacts,
                )
                rollout_futures.append(submission)
                future_to_scene[submission] = scenario.scene_id
                n_rollouts_to_dispatch -= 1

        for rollout_future in rollout_futures:
            try:
                rollouts.append(rollout_future.result())
            except Exception as exc:
                logger.warn(f"error with {future_to_scene[rollout_future]}")
                message = f"Error with scenario.scene_id = {future_to_scene[rollout_future]}:\n"
                message += "\n".join(traceback.format_exception(exc))
                error_messages.append(
                    message
                )  # since error_messages is not empty, we will raise in a moment

    if error_messages:
        raise AssertionError("\n".join(error_messages))

    start_time = time.perf_counter()
    success = await dispatcher.dispatch_rollouts(rollouts)
    total_time = time.perf_counter() - start_time
    logger.info(
        "Simulated %d rollouts in %.2f seconds, i.e. %.2f seconds per rollout",
        len(rollouts),
        total_time,
        total_time / len(rollouts) if len(rollouts) > 0 else 0,
    )

    if args.prometheus_out is not None:
        try:
            dump_prometheus_metrics(args.prometheus_out)
        except Exception as e:
            logger.warn(
                f"Failed to write prometheus metrics to disk (to location: {args.prometheus_out}). "
                f"Error: {str(e)}. The test can still pass."
            )

    return success


if __name__ == "__main__":
    parser = create_arg_parser()
    args = parser.parse_args()

    success = asyncio.run(aio_main(args))
    logging.info("Alpasim finished.")

    sys.exit(0 if success else 1)
